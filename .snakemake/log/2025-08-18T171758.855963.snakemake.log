Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 64
Rules claiming more threads will be scaled down.
Job stats:
job           count
----------  -------
graft_its         1
its_bridge        1
qc_locus          1
total             3

Select jobs to execute...

[Mon Aug 18 17:17:59 2025]
rule qc_locus:
    input: data/staging/ITS.gb
    output: data/qc/ITS.cleaned.fasta
    jobid: 39
    reason: Missing output files: data/qc/ITS.cleaned.fasta
    wildcards: locus=ITS
    resources: tmpdir=/tmp

[Mon Aug 18 17:18:00 2025]
Error in rule qc_locus:
    jobid: 39
    input: data/staging/ITS.gb
    output: data/qc/ITS.cleaned.fasta
    shell:
        mkdir -p data/qc && python3.11 workflow/scripts/normalize_ids.py --in data/staging/ITS.gb --locus ITS --out data/qc/ITS.cleaned.fasta
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2025-08-18T171758.855963.snakemake.log
